import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import time

# åœ–æ›¸é¤¨åƒæ•¸
WIDTH, HEIGHT = 50, 30  # åœ–æ›¸é¤¨å°ºå¯¸ï¼ˆç±³ï¼‰
NUM_DEVICES = 15  # è—èŠ½è£ç½®æ•¸é‡
NUM_SAMPLES = 10_000_000  # è¨“ç·´æ•¸æ“šæ¨£æœ¬æ•¸
P_t = -20  # ç™¼å°„åŠŸç‡ (dBm)
n = 2.5  # è·¯å¾‘æè€—æŒ‡æ•¸
A = 10  # ç’°å¢ƒè¡°æ¸› (dBm)
NOISE_STD = 3  # é«˜æ–¯å™ªè²æ¨™æº–å·® (dBm)

def simulate_rssi_vectorized(positions, device_positions, shelves=None):
    """è¨ˆç®—å‘é‡åŒ– RSSIï¼ˆåŒ…å«å™ªè²ã€æ›¸æ¶é®æ“‹å’Œå¤šè·¯å¾‘æ•ˆæ‡‰ï¼‰"""
    diff = positions[:, np.newaxis, :] - device_positions[np.newaxis, :, :]
    distance = np.linalg.norm(diff, axis=2)
    distance = np.maximum(distance, 0.1)
    rssi = P_t - 10 * n * np.log10(distance) - A

    # æ¨¡æ“¬æ›¸æ¶é®æ“‹
    if shelves is not None:
        for shelf in shelves:
            shelf_x, shelf_y = shelf['position']
            shelf_w, shelf_h = shelf['size']
            for i in range(len(positions)):
                for j in range(len(device_positions)):
                    if (min(positions[i, 0], device_positions[j, 0]) < shelf_x < max(positions[i, 0], device_positions[j, 0]) and
                        min(positions[i, 1], device_positions[j, 1]) < shelf_y < max(positions[i, 1], device_positions[j, 1])):
                        rssi[i, j] -= 7  # æ›¸æ¶é®æ“‹è¡°æ¸›

    # æ¨¡æ“¬å¤šè·¯å¾‘æ•ˆæ‡‰ï¼ˆéš¨æ©Ÿåå°„ï¼‰
    multipath = np.random.uniform(0, 3, rssi.shape)  # é¡å¤–è¡°æ¸› 0-3 dBm
    rssi -= multipath

    # æ··åˆå™ªè²ï¼šé«˜æ–¯å™ªè² + çªç™¼å™ªè²
    gaussian_noise = np.random.normal(0, NOISE_STD, rssi.shape)
    burst_noise = np.random.choice([0, 7], size=rssi.shape, p=[0.9, 0.1])  # 10% æ¦‚ç‡çªç™¼å™ªè²
    rssi += gaussian_noise + burst_noise
    return rssi.astype(np.float32)

def generate_dataset_batched(num_samples, batch_size=500_000):
    """åˆ†æ‰¹ç”Ÿæˆå¤§è¦æ¨¡ RSSI æ•¸æ“šé›†ï¼Œç¢ºä¿å‡å‹»åˆ†ä½ˆï¼Œä¸¦ç”Ÿæˆç½®ä¿¡åº¦æ¨™ç±¤"""
    device_positions = np.random.uniform([0, 0], [WIDTH, HEIGHT], (NUM_DEVICES, 2)).astype(np.float32)
    # æ¨¡æ“¬æ›¸æ¶
    shelves = [
        {'position': [10, 15], 'size': [10, 1]},
        {'position': [20, 15], 'size': [10, 1]},
        {'position': [30, 15], 'size': [10, 1]},
        {'position': [40, 15], 'size': [10, 1]}
    ]
    data_list, label_list = [], []

    # åˆ†å±¤æ¡æ¨£ï¼š5m x 5m ç¶²æ ¼
    grid_size = 5
    samples_per_grid = num_samples // ((WIDTH // grid_size) * (HEIGHT // grid_size))
    for i in range(0, int(WIDTH), grid_size):
        for j in range(0, int(HEIGHT), grid_size):
            positions = np.random.uniform([i, j], [i + grid_size, j + grid_size], (samples_per_grid, 2)).astype(np.float32)
            rssi_data = simulate_rssi_vectorized(positions, device_positions, shelves)
            # ç‚ºæ¨™ç±¤æ·»åŠ ç½®ä¿¡åº¦åˆ—ï¼ˆè¨­ç‚º 1.0ï¼‰
            confidences = np.ones((len(positions), 1), dtype=np.float32)
            labels_with_confidence = np.hstack((positions, confidences))
            data_list.append(rssi_data)
            label_list.append(labels_with_confidence)

    return np.vstack(data_list), np.vstack(label_list), device_positions

def train_cnn_model(data, labels):
    """è¨“ç·´å¢å¼·çš„ CNN æ¨¡å‹"""
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    data_reshaped = data_scaled.reshape(-1, 5, 3, 1)

    X_train, X_val, y_train, y_val = train_test_split(data_reshaped, labels, test_size=0.2, random_state=42)

    model = Sequential([
        Input(shape=(5, 3, 1)),  # é¡¯å¼æŒ‡å®šè¼¸å…¥å½¢ç‹€
        Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
        BatchNormalization(),
        Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
        BatchNormalization(),
        Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
        BatchNormalization(),
        Flatten(),
        Dense(512, activation='relu'),
        Dropout(0.3),
        Dense(256, activation='relu'),
        Dropout(0.2),
        Dense(3)  # è¼¸å‡º (x, y, confidence)
    ])

    model.compile(optimizer='adam', loss='mse')
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=512, callbacks=[reduce_lr], verbose=1)
    return model, scaler, history

def simulate_user_path(num_points=100):
    """æ¨¡æ“¬ç”¨æˆ¶ç§»å‹•è·¯å¾‘ï¼ˆæ©¢åœ“è·¯å¾‘ï¼‰"""
    t = np.linspace(0, 2 * np.pi, num_points)
    x = 25 + 15 * np.cos(t)
    y = 15 + 10 * np.sin(t)
    return np.vstack((x, y)).T

def predict_user_path(model, scaler, path, device_positions):
    """é æ¸¬ç”¨æˆ¶ä½ç½®ä¸¦å¹³æ»‘è·¯å¾‘"""
    rssi_data = np.array([simulate_rssi_vectorized(np.array([pos]), device_positions)[0] for pos in path])
    rssi_scaled = scaler.transform(rssi_data)
    rssi_reshaped = rssi_scaled.reshape(-1, 5, 3, 1)
    predictions = model.predict(rssi_reshaped, verbose=0)

    # åˆ†é›¢åº§æ¨™å’Œç½®ä¿¡åº¦
    predicted_positions = predictions[:, :2]
    confidences = predictions[:, 2]

    # ç½®ä¿¡åº¦éæ¿¾ï¼ˆä½æ–¼ 0.5 çš„é»ä½¿ç”¨ç§»å‹•å¹³å‡æ›¿ä»£ï¼‰
    smoothed_positions = predicted_positions.copy()
    window_size = 5
    for i in range(len(predicted_positions)):
        if i >= window_size and confidences[i] < 0.5:
            smoothed_positions[i] = np.mean(predicted_positions[max(0, i-window_size):i], axis=0)

    # ç§»å‹•å¹³å‡å¹³æ»‘
    for i in range(len(smoothed_positions)):
        start = max(0, i - window_size // 2)
        end = min(len(smoothed_positions), i + window_size // 2 + 1)
        smoothed_positions[i] = np.mean(smoothed_positions[start:end], axis=0)

    return smoothed_positions, confidences

def plot_results(actual_path, predicted_path, device_positions, confidences, history):
    """ç¹ªè£½çµæœåœ–è¡¨ä¸¦ä¿å­˜è¨“ç·´æå¤±æ›²ç·š"""
    # ç¹ªè£½è·¯å¾‘
    plt.figure(figsize=(12, 8))
    plt.scatter(device_positions[:, 0], device_positions[:, 1], c='red', marker='^', s=100, label='BLE Devices')
    plt.plot(actual_path[:, 0], actual_path[:, 1], 'b-', label='Actual Path')
    plt.plot(predicted_path[:, 0], predicted_path[:, 1], 'r--', label='Predicted Path')
    plt.title('Actual vs Predicted User Path in Library')
    plt.xlabel('X (m)')
    plt.ylabel('Y (m)')
    plt.legend()
    plt.grid(True)
    plt.xlim(0, WIDTH)
    plt.ylim(0, HEIGHT)
    plt.savefig('location_prediction.png')
    plt.close()

    # ç¹ªè£½è¨“ç·´æå¤±æ›²ç·š
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss (MSE)')
    plt.legend()
    plt.grid(True)
    plt.savefig('training_loss.png')
    plt.close()

if __name__ == "__main__":
    total_start = time.time()

    print("ğŸš€ é–‹å§‹è³‡æ–™ç”¢ç”Ÿ...")
    start = time.time()
    data, labels, device_positions = generate_dataset_batched(NUM_SAMPLES)
    print(f"âœ… è³‡æ–™ç”¢ç”Ÿå®Œæˆï¼Œç”¨æ™‚ {time.time() - start:.2f} ç§’")

    print("ğŸš€ é–‹å§‹æ¨¡å‹è¨“ç·´...")
    start = time.time()
    model, scaler, history = train_cnn_model(data, labels)
    print(f"âœ… æ¨¡å‹è¨“ç·´å®Œæˆï¼Œç”¨æ™‚ {time.time() - start:.2f} ç§’")

    print("ğŸš€ æ¨¡æ“¬ç”¨æˆ¶ç§»å‹•ä¸¦é æ¸¬ä½ç½®...")
    start = time.time()
    actual_path = simulate_user_path()
    predicted_path, confidences = predict_user_path(model, scaler, actual_path, device_positions)
    print(f"âœ… ä½ç½®é æ¸¬å®Œæˆï¼Œç”¨æ™‚ {time.time() - start:.2f} ç§’")

    print("ğŸš€ ç¹ªè£½çµæœåœ–è¡¨...")
    start = time.time()
    plot_results(actual_path, predicted_path, device_positions, confidences, history)
    print(f"âœ… ç¹ªåœ–å®Œæˆï¼Œç”¨æ™‚ {time.time() - start:.2f} ç§’")

    errors = np.sqrt(np.sum((predicted_path - actual_path) ** 2, axis=1))
    print(f"ğŸ“ å¹³å‡å®šä½èª¤å·®: {np.mean(errors):.2f} ç±³")
    print(f"ğŸ“‰ æœ€å¤§èª¤å·®: {np.max(errors):.2f} ç±³")
    print(f"ğŸ“ˆ æœ€å°èª¤å·®: {np.min(errors):.2f} ç±³")
    print(f"ğŸ å…¨éƒ¨æµç¨‹ç¸½è€—æ™‚: {time.time() - total_start:.2f} ç§’")